{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Retrieval-Augmented Generation (RAG) enabled GenAI Application with LlamaIndex</h1>\n",
    "\n",
    "<h3><b>Varad V. Deshmukh</b></h3>\n",
    "\n",
    "_Data Scientist_ - _Machine Learning / MLOps Engineer_ - _AI Prompt Engineer_</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **What is RAG?**\n",
    "\n",
    "__Large Language Models__ (LLMs) are trained on an enormous corpus of data, e.g. Wikipedia pages, software/hardware technical documentation, blogs, etc. But, they are not trained on our personal data. One of the solutions to this issue is to fine-tune the model with our data. This involves altering the neural network architecture which lies underneath the model, including adding some layers and removing some. This altered model is then trained on our data. This appears as a promising approach, but has certain drawbacks - \n",
    "\n",
    "1. Training an LLM is expensive.\n",
    "2. Due to the high costs associated with training them, its very difficult to continually update them with the latest data.\n",
    "3. Observability is lacking, implying that we have no means to peek into the process by which the model arrived at the response.\n",
    "\n",
    "__Retrieval-Augmented Generation__ (RAG) is an alternative, transformative paradigm that works around these problems. Instead of asking the model to generate the response directly, RAG first retrieves information from our data sources, adds it to the context of our query and then asks the model to answer the query based on the enriched prompt. RAG overcomes all three weaknesses of the fine-tuning approach -\n",
    "\n",
    "1. There is no training involved, so its cheap.\n",
    "2. Data is fetched only when you ask for it, so it is always up to date.\n",
    "3. We can see the documents from where the model retrieved its response, so it is trustworthy.\n",
    "\n",
    "RAG adds our data - stored in varied formats as text documents, pdfs, images, videos, audios, etc. - to the data LLMs already have access to. Our data is loaded and prepared for queries, i.e. 'indexed'. Almost always, this entails converting it to vector embeddings, which are numerical representations of the data concerned. User queries, or the questions that we want the model to respond to, generally through prompts, act on this indexed data. The RAG approach filters down the data down to the most relevant context, i.e. it chooses which document sources are the most relevant to answer the question at hand. This context and our query then go to the LLM in the form of a prompt, to which the model generates a response.\n",
    "\n",
    "> ### **Stages within RAG**\n",
    "\n",
    "There are five key stages within RAG, which are to be incorporated into any LLM application that we build -\n",
    "\n",
    "1. __Loading__ : getting our data from where it lives - text files, PDFs, a website, a database or an API - into the pipeline\n",
    "\n",
    "2. __Indexing__ : converting the data into a format suitable for querying, which almost always is a vector embedding, incorporating the semantics of our data as well as the necessary metadata\n",
    "\n",
    "3. __Storing__ : storing the indexed data, i.e. the embeddings into a vector database, to avoid having to re-index it\n",
    "\n",
    "4. __Querying__ : prompting the RAG-enabled model to answer a specific user query, to which it returns a context-aware response, along with the citations to the source documents\n",
    "\n",
    "5. __Evaluation__ : checking the efficacy of the RAG pipeline and objectively measuring how accurate, failthful and fast the model responses are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary modules\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    OpenAIEmbedding,\n",
    "    PromptHelper,\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the OpenAI API key\n",
    "# replace with your own API key\n",
    "os.environ['OPENAI_API_KEY'] = 'your_api_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "document_directory = '/Users/varad/Desktop/rck/data/'\n",
    "documents = SimpleDirectoryReader(input_dir=document_directory).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the LLM and the embedding model\n",
    "llm = OpenAI(\n",
    "  model='gpt-3.5-turbo',\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customise the embedding model\n",
    "prompt_helper = PromptHelper(\n",
    "  context_window=4096, \n",
    "  num_output=256, \n",
    "  chunk_overlap_ratio=0.1, \n",
    "  chunk_size_limit=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes out of text chunks\n",
    "text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "  llm=llm,\n",
    "  embed_model=embed_model,\n",
    "  text_splitter=text_splitter,\n",
    "  prompt_helper=prompt_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the vector embeddings into a vector store\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context = service_context\n",
    "    )\n",
    "# save the embeddings to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the query engine\n",
    "query_engine = index.as_query_engine(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for questioning the model\n",
    "def ask_the_model(prompt):\n",
    "    response = query_engine.query(\n",
    "        prompt,\n",
    "        similarity_top_k=4\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the question\n",
    "prompt = '''\n",
    "Your Prompt here.\n",
    "'''\n",
    "\n",
    "response = ask_the_model(prompt)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the citations from the relevant source documents\n",
    "response.get_formatted_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thank you for going through the notebook. If you liked it, please give a star!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
