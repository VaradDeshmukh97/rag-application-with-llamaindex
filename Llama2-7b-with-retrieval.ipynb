{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>RAG-Fin-GPT : An AI Tool for Financial Research and Analytics</b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an AI solution for performing in-depth financial research and analysis. This system is based on Retrieval-Augmented Generation (RAG), utilizing a locally run Llama2-7b-chat LLM, develoepd by Meta. This system uses completely open-source components and takes care of the data security considerations as well, by hosting everything on a local system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>------------    HuggingFace CLI Login and Module Imports    ------------</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "from huggingface_hub import login\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "#from llama_index.core.llms.utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core import ServiceContext, SimpleDirectoryReader, VectorStoreIndex, set_global_service_context, set_global_tokenizer, StorageContext, load_index_from_storage\n",
    "\n",
    "from llama_index.core import download_loader\n",
    "from llama_index.readers.web import NewsArticleReader\n",
    "\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_response, display_source_node, display_query_and_multimodal_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\rck05\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_token = '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'\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>------------    Logging    ------------</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    stream = sys.stdout,\n",
    "    level = logging.INFO\n",
    ")\n",
    "logging.getLogger().addHandler(\n",
    "    logging.StreamHandler(\n",
    "        stream = sys.stdout\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>------------    Large Language Models (LLMs)    ------------</b></center>\n",
    "\n",
    "We are using locally running open-source LLMs for our system. The details are as follows.\n",
    "\n",
    "* Foundational Model : **Llama2-7b-chat**\n",
    "* Tokenizer model : **Llama2-7b-chat _(tokenizer)_**\n",
    "* Embedding model : **WhereIsAI/UAE-Large-V1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Llama2-7b'\n",
    "model_path = r\"D:\\0-VARAD-DESHMUKH\\models\\llama-2-7b-chat.Q6_K.gguf\"\n",
    "max_new_tokens = 2048\n",
    "context_window = 4096\n",
    "\n",
    "system_prompt = '''\n",
    "You are an experienced investment and financial research analyst, who always generates responses based only on the source documents given./\n",
    "You cite the relevant source documents properly at the end of the response or in the format 'According to <source>,'. You include the numerical figures/\n",
    "from the source documents to elucidate your response, but NEVER HALLUCINATE ANY INFORMATION. If any details are missing from the source documents,/\n",
    "you explicitly state so, rather than making up the missing information. Your responses are well-cited and credible, apt to be included in research reports.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from D:\\0-VARAD-DESHMUKH\\models\\llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    17.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   288.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-generation model \"Llama2-7b\" loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '18', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "llm = LlamaCPP(\n",
    "    model_path = model_path,\n",
    "    temperature = 0,\n",
    "    max_new_tokens = max_new_tokens,\n",
    "    context_window = context_window,\n",
    "    generate_kwargs = {},\n",
    "    model_kwargs = {\n",
    "        'load_in_8bit' : True,\n",
    "        #'n_gpu_layers' : -1\n",
    "    },\n",
    "    system_prompt = system_prompt,\n",
    "    #messages_to_prompt=messages_to_prompt,\n",
    "    #completion_to_prompt=completion_to_prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "print('Text-generation model \"Llama2-7b\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = r'meta-llama/Llama-2-7b-chat-hf'\n",
    "hf_token = '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'\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=tokenizer_model,\n",
    "        token=hf_token\n",
    "    ).encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model found in cache.\n",
      "Model name:  WhereIsAI/UAE-Large-V1 \n",
      "Model Directory:  C:\\Users\\rck05\\.cache\\huggingface\\hub\\models--WhereIsAI--UAE-Large-V1\\snapshots\\82f6ace7a8954c012dd2ae05e2604fbc9007205b\n"
     ]
    }
   ],
   "source": [
    "embed_model_path = r\"C:\\Users\\rck05\\.cache\\huggingface\\hub\\models--WhereIsAI--UAE-Large-V1\\snapshots\\82f6ace7a8954c012dd2ae05e2604fbc9007205b\"\n",
    "embed_model_name = 'WhereIsAI/UAE-Large-V1'\n",
    "\n",
    "if not os.path.exists(embed_model_path):\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        embed_model_name\n",
    "    )\n",
    "    print('Embedding model not found in cache. Downloading and creating one.!')\n",
    "else:\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        embed_model_path\n",
    "    ) \n",
    "    print('Embedding model found in cache.')\n",
    "\n",
    "print('Model name: ', embed_model_name, '\\nModel Directory: ', embed_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>------------    Global Settings    ------------</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings done.\n"
     ]
    }
   ],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "#Settings.chunk_size = 512\n",
    "Settings.context_window = context_window\n",
    "Settings.num_output = max_new_tokens\n",
    "\n",
    "print('Settings done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>------------    Data Loading    ------------</b></center>\n",
    "\n",
    "We load the source documents into a local directory. The source documents could be:\n",
    "1. Local PDFs\n",
    "2. News Articles\n",
    "3. Websites\n",
    "4. Static HTMLs - SEC filings, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local PDFs\n",
    "\n",
    "document_directory = r\"D:\\0-VARAD-DESHMUKH\\Files\\data\"\n",
    "\n",
    "pdfs = SimpleDirectoryReader(\n",
    "    document_directory,\n",
    "    filename_as_id=True\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Articles\n",
    "\n",
    "news_articles = [\n",
    "    r'https://www.indiatvnews.com/technology/news/meta-collaborates-with-ncmec-to-extend-take-it-down-program-for-teenagers-2024-02-07-915677',\n",
    "    r'https://www.msn.com/en-in/money/news/meta-to-label-ai-generated-images-across-social-media-platforms-details-here/ar-BB1hTNrL',\n",
    "    r'https://www.msn.com/en-in/money/other/meta-announces-plans-to-combat-deepfakes-and-ai-generated-content-on-facebook-instagram-threads-ahead-of-key-elections/ar-BB1hTfPt',\n",
    "    r'https://timesofindia.indiatimes.com/gadgets-news/20-years-of-facebook-meta-added-more-than-one-tcs-in-a-day-to-its-value/articleshow/107460150.cms',\n",
    "    r'https://www.nytimes.com/2024/02/01/technology/meta-profit-report.html',\n",
    "    r'https://www.msn.com/en-in/money/markets/meta-platforms-shatters-records-with-a-196-bn-surge-in-stock-market-value/ar-BB1hMN6e',\n",
    "]\n",
    "\n",
    "news_reader = NewsArticleReader(use_nlp=False)\n",
    "news = news_reader.load_data(\n",
    "    news_articles\n",
    ")\n",
    "\n",
    "# change 'publish_date' metadata to string for JSON serialization\n",
    "for i in range(len(news)):\n",
    "    news[i].metadata['publish_date'] = str(news[i].metadata['publish_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Websites\n",
    "\n",
    "WholeSiteReader = download_loader('WholeSiteReader')\n",
    "\n",
    "prefix = r'https://about.meta.com'\n",
    "base_url = r'https://about.meta.com/company-info/'\n",
    "max_depth = 1\n",
    "\n",
    "scraper = WholeSiteReader(\n",
    "    prefix=prefix,\n",
    "    max_depth=max_depth\n",
    ")\n",
    "\n",
    "websites = scraper.load_data(\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static htmls : SEC filings, etc.\n",
    "\n",
    "SimpleWebPageReader = download_loader('SimpleWebPageReader')\n",
    "\n",
    "urls = [\n",
    "    r'https://www.sec.gov/Archives/edgar/data/1326801/000132680124000012/meta-20231231.htm'\n",
    "]\n",
    "loader = SimpleWebPageReader()\n",
    "\n",
    "htmls = loader.load_data(\n",
    "    urls=urls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating all the sources into Document objects\n",
    "documents = pdfs + news + websites + htmls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdced6808244cb2bfd2a41bb2eeda18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dada5e71614bad8148902d7338689b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9532a246cc55419086aedfcb1ff4d39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e691a2ff8f48f8aa206acca678167a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b606b0b2160641799cd20c399ea3fc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821ba3a05c334b5b9356be8947d15907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7f54903afa41c8bc13cc85276b56b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dca9b59a3054e3eb5c94d63d020618b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d758c88e2c842dd82b1c99618eafcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3e7b485cce44d79803dd6648e20eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957c06b1f9904cb5861f1bf0e046ed01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0444b7b05c4cdf8c55988365e305f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fd04afa6e741e9859c1a34b3ce859b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4c2ecc80fa4bfba04770c77d86eacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22112f06aed7419e9b832edf7c6195ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf6121e9a5e4d60ad8843e907b316e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aa9b3bc70d41f8a06cacabfaf20d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d473824e7c74445e9fb6d1ffdd4d55d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2efd445f0f4863852705b8b8323f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents = pdfs + news\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "    \n",
    "embedding = HuggingFaceEmbedding(embed_model_name)\n",
    "    \n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[splitter, embedding]\n",
    ")\n",
    "    \n",
    "nodes = pipeline.run(\n",
    "        documents=documents,\n",
    "        in_place=False,\n",
    "        show_progress=True\n",
    ")\n",
    "   \n",
    "    # load the documents and create the index\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "temp_index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>------------    Data Ingestion and Indexing Pipeline    ------------</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Embeddings found in cache. Loaded directly.\n"
     ]
    }
   ],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "    \n",
    "    embedding = HuggingFaceEmbedding(embed_model_name)\n",
    "    \n",
    "    pipeline = IngestionPipeline(\n",
    "    transformations=[splitter, embedding]\n",
    ")\n",
    "    \n",
    "    nodes = pipeline.run(\n",
    "        documents=documents,\n",
    "        in_place=False,\n",
    "        show_progress=True\n",
    ")\n",
    "   \n",
    "    # load the documents and create the index\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    \n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "    print('Documents embedded and loaded into memory.')\n",
    "\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    nodes = list(index.docstore.docs.values())\n",
    "    print('Embeddings found in cache. Loaded directly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid retriever + reranking\n",
    "vector_retriever = temp_index.as_retriever(\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "class Hybridretriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node_id)\n",
    "        \n",
    "        return all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrievers\n",
    "temp_index.as_retriever(similarity_top_k=3)\n",
    "hybrid_retriever = Hybridretriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node postprocessing\n",
    "\n",
    "# 1. re-ranking\n",
    "reranker = SentenceTransformerRerank(\n",
    "    top_n=3,\n",
    "    model='BAAI/bge-reranker-base'\n",
    ")\n",
    "\n",
    "# 2. filtering out irrelevant nodes\n",
    "filter = SimilarityPostprocessor(\n",
    "    similarity_cutoff=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b1e48b02a4421f9ff7d961f942cd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = '''\n",
    "Discuss how Meta plans to tackle deepfakes and AI-generated content ahead of the upcoming elections. Give a detailed overview of the initiatives Meta is taking towards,/\n",
    "adopting responsible business practices, according to the source documents. You have to prove that your response is correct by citing the relevant sections from the source documents./\n",
    "Cross-check your response for factual accuracy and correct it, if needed. Your response must not contain any information that is not present in the source document./\n",
    "Structure your output as a paragraph under 500 words. FOLLOW ALL THE INSTRUCTIONS CAREFULLY.'''\n",
    "\n",
    "retrieved_nodes = hybrid_retriever.retrieve(prompt)\n",
    "\n",
    "reranked_nodes = reranker.postprocess_nodes(\n",
    "    retrieved_nodes,\n",
    "    query_bundle=QueryBundle(prompt)\n",
    ")\n",
    "\n",
    "filtered_nodes = filter.postprocess_nodes(\n",
    "    reranked_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 11d0ea4e-a3b6-4d71-9715-6afc59c918c1\n",
      "Text:\n",
      "Score:  0.938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# source nodes\n",
    "for node in filtered_nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker, filter],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994c123fb9174c88a3d4edb43d395166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   52692.67 ms\n",
      "llama_print_timings:      sample time =      98.45 ms /   415 runs   (    0.24 ms per token,  4215.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16150.54 ms /   163 tokens (   99.08 ms per token,    10.09 tokens per second)\n",
      "llama_print_timings:        eval time =  107560.65 ms /   414 runs   (  259.81 ms per token,     3.85 tokens per second)\n",
      "llama_print_timings:       total time =  125001.72 ms /   577 tokens\n"
     ]
    }
   ],
   "source": [
    "# response generation\n",
    "response = query_engine.query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Meta has announced plans to combat deepfakes and AI-generated content ahead of the upcoming elections. According to the source document, Meta is taking several initiatives towards adopting responsible business practices. Firstly, Meta is implementing a new policy that prohibits any content that is manipulated or fabricated using AI technology. This includes deepfakes, which are manipulated videos or images that appear real but have been altered using AI. Secondly, Meta is using AI-powered tools to detect and flag any content that violates its policy. These tools will be able to identify deepfakes and other manipulated content more accurately than human moderators. Thirdly, Meta is partnering with fact-checking organizations to help identify and flag false information. This includes content that is not necessarily manipulated using AI but is still false or misleading. Finally, Meta is increasing its investment in AI-powered content moderation tools. This will help the company to more effectively identify and remove manipulated content from its platforms. According to the source document, Meta is committed to ensuring that its platforms are a safe and secure environment for users ahead of the upcoming elections. By implementing these initiatives, Meta aims to prevent the spread of misinformation and manipulated content that could potentially influence the outcome of the elections. \n",
       "Citations:\n",
       "1. Meta. (n.d.). Meta Announces Plans to Combat Deepfakes and AI-Generated Content Ahead of Key Elections. Retrieved from <https://www.msn.com/en-in/money/other/meta-announces-plans-to-combat-deepfakes-and-ai-generated-content-on-facebook-instagram-threads-ahead-of-key-elections/ar-BB1hTfPt>\n",
       "Note: All the citations provided are accurate and have been formatted according to the required format."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/1`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 11d0ea4e-a3b6-4d71-9715-6afc59c918c1<br>**Similarity:** 0.9383050799369812<br>**Text:** <br>**Metadata:** {'title': 'MSN', 'link': 'https://www.msn.com/en-in/money/other/meta-announces-plans-to-combat-deepfakes-and-ai-generated-content-on-facebook-instagram-threads-ahead-of-key-elections/ar-BB1hTfPt', 'authors': [], 'language': 'en', 'description': '', 'publish_date': 'None'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(\n",
    "    response=response,\n",
    "    show_source=True,\n",
    "    show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query engine - streaming\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker, filter],\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response generation\n",
    "response = query_engine.query(prompt)\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "############ TREE-SUMMARIZE ############\n",
    "########################################\n",
    "\n",
    "\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "summarizer = TreeSummarize(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Major Milestones\n",
    "The \"Nifty 50\" refers to the National Stock Exchange of India's benchmark index, Nifty 50. It comprises 50 of the large and liquid stocks traded on the NSE. The index has hit several major milestones through the years. Here are some of the significant milestones for the Nifty 50\n",
    "\n",
    "Introduction of Nifty 50 : The Nifty 50 index was introduced by the National Stock Exchange of India (NSE) on April 22, 1996. It provided a benchmark for investors to track the performance of the Indian stock market.\n",
    "\n",
    "Nifty at 1000: The Nifty 50 index started with a base value of 1000.\n",
    "\n",
    "Nifty at 2,000: The Nifty scaled the first 1000 points in 8 years. It touched 2000 on December 2, 2004.\n",
    "\n",
    "Nifty at 3,000: However it took the Nifty a little over a year to scale the next 1000 points from 2000 to 3000. It hit the 3000 mark on January 30, 2006.\n",
    "\n",
    "Global Financial Crisis: Just like all other key global indices, the index faced a major setback during the global financial crisis in 2008. It fell significantly, mirroring the impact of the crisis on the Indian economy and financial markets.\n",
    "\n",
    "Crossing 10,000 Points: The Nifty 50 index crossed the significant milestone of 10,000-point mark on July 25, 2017. This was on the back of several reforms undertaken to improve the country’s financial health coupled with RBI Policy push and favourable monsoon.\n",
    "\n",
    "Covid-19 Pandemic: The Covid-19 pandemic in 2020 led to extreme volatility in global financial markets, including the Nifty 50. The index experienced a sharp decline in February-March 2020 but recovered significantly in the latter half of the year.\n",
    "\n",
    "Technology and Pharma Sector Surge: In the wake of the pandemic, there was a notable surge in technology and pharmaceutical sectors. These sectors played a crucial role in the market recovery in 2020-2021.\n",
    "\n",
    "Market Capitalization Milestones: The index has seen various companies become India's largest by market capitalization over different periods. Companies like Reliance Industries, Tata Consultancy Services (TCS), and HDFC Bank have taken turns as the largest companies by market cap.\n",
    "\n",
    "Sectoral Changes: The composition of the Nifty 50 index is periodically reviewed and updated to reflect the changing dynamics of the Indian economy. Companies from different sectors have been added or removed from the index based on their market performance.\n",
    "\n",
    "Nifty 50 major timelines\n",
    "\n",
    "Nifty at 1,000: The index was launched with a base value of 1,000 in 1996.\n",
    "\n",
    "Nifty at 2,000: It touched 2000 on December 2, 2004.\n",
    "\n",
    "Nifty at 3,000: Nifty zoomed past the 3,000 mark on January 30, 2006.\n",
    "\n",
    "Nifty at 4,000: Nifty crossed the 4000 mark on December 1, 2006.\n",
    "\n",
    "Nifty at 5,000: Nifty sailed to 5,000 on September 27, 2007.\n",
    "\n",
    "Nifty at 6000: Nifty ended 2007 with another 1000-point gain.It hit 6000 onDecember 11, 2007.\n",
    "\n",
    "Nifty at 7,000: The journey from 6000 to 7000 took 7 years. Nifty touched 7,000 on May 12, 2014.\n",
    "\n",
    "Nifty at 8,000: The 2014 rally continued, Nifty hit 8,000 on September 1, 2014.\n",
    "\n",
    "Nifty at 9,000: In 2017, Nifty touched the 9,000-mark on March 14, 2017.\n",
    "\n",
    "Nifty at 10,000: The Nifty hit the psychologically important 10,000 mark on July 25, 2017.\n",
    "\n",
    "Nifty at 11,000: On January 23, 2018, Nifty hots 11,000 first time ever.\n",
    "\n",
    "Nifty at 12,000: On June 3, 2019, Nifty closed above 12000 for the first time.\n",
    "\n",
    "Pandemic Low: Nifty slumped to 7,511 on March 24, 2020.\n",
    "\n",
    "Nifty at 13,000: Nifty hit 13,000 after a steady recovery from the Pandemic lows and hot 13,000 for first time on November 24, 2020.\n",
    "\n",
    "Nifty at 14,000: On the last trading day of 2020- December 31, Nifty hit 14,000.\n",
    "\n",
    "Nifty at 15,000: On February 6,2021, Nifty touches 15,000 for the first time.\n",
    "\n",
    "Nifty at 16,000: Nifty scales past 16,000 on August 3, 2021.\n",
    "\n",
    "Nifty at 17,000: Nifty however crossed the next 1000 points in matter of 28 days and hit 17,000 on August 31, 2021. Nifty at 18,0000:Nifty 50 covered the distance to 18,000 in 40 days on October 11, 2021.\n",
    "\n",
    "Nifty at 19,000: Nifty hit 19,000 for the first time on June 28, 2023, almost 21 months after scaling the 18,000 mark.\n",
    "\n",
    "'''\n",
    "\n",
    "response = summarizer.get_response('What is Nifty and how did it evolve over time?', text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on a labelled dataset\n",
    "\n",
    "%pip install llama-index-packs-rag-evaluator\n",
    "\n",
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "# download a LabelledRagDataset from llama-hub\n",
    "rag_dataset, eval_documents = download_llama_dataset(\n",
    "    \"PaulGrahamEssayDataset\", \"./paul_graham\"\n",
    ")\n",
    "\n",
    "# build a basic RAG pipeline off of the source documents\n",
    "eval_index = VectorStoreIndex.from_documents(documents=eval_documents)\n",
    "eval_query_engine = index.as_query_engine()\n",
    "\n",
    "# Time to benchmark/evaluate this RAG pipeline\n",
    "# Download and install dependencies\n",
    "RagEvaluatorPack = download_llama_pack(\n",
    "    \"RagEvaluatorPack\", \"./rag_evaluator_pack\"\n",
    ")\n",
    "\n",
    "# construction requires a query_engine, a rag_dataset, and optionally a judge_llm\n",
    "rag_evaluator_pack = RagEvaluatorPack(\n",
    "    query_engine=eval_query_engine, rag_dataset=rag_dataset\n",
    ")\n",
    "\n",
    "# PERFORM EVALUATION\n",
    "benchmark_df = rag_evaluator_pack.run()  # async arun() also supported\n",
    "print(benchmark_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
