{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>`RCK-GPT`</b> : A Production-grade RAG pipeline for Financial Analysis</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain vs. LlamaIndex\n",
    "\n",
    "\n",
    "**Creating a chatbot with a local LLM**\n",
    "\n",
    "Choose to keep the LLM running in a standalone inference server, instead of loading it into memory every single time you run the scripts. This saves time and avoids wearing down the disk. We do this via a OpenAI-compatible API schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamaindex\n",
    "from llama_index.llms import ChatMessage, OpenAILike\n",
    "\n",
    "llm = OpenAILike(\n",
    "    api_base = 'https://localhost:1234/v1',\n",
    "    timeout = 600,\n",
    "    api_key = 'loremIpsum',\n",
    "    is_chat_model = True,   # chat or completion?\n",
    "    context_window = 32768\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(\n",
    "        role = 'system',\n",
    "        content = system_prompt\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role = 'user',\n",
    "        content = user_prompt,\n",
    "    )\n",
    "]\n",
    "\n",
    "output = llm.chat(chat_history)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Using LLMs<h1></center>\n",
    "\n",
    "You can use more than one also. They are used in multiple stages :\n",
    "1. Indexing - to determine relevance or summarize and then index the summaries\n",
    "2. Querying, in two ways - (i) Retrieval - array of options such as multiple different indices and make decisions about where best to find the information you are looking for, an agentic LLM can also use tools to query different data sources, (ii) Response synthesis - an LLM can combine answers to multiple sub-queries into a single coherent answer or can transform data\n",
    "\n",
    "LlamaIndex provides a single interface to a large number of different LLMs, allowing you to pass in any LLM you choose to any stage of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using OpenAI\n",
    "from llama_index import OpenAI\n",
    "\n",
    "response = OpenAI().complete('Paul Graham is ')\n",
    "print(response)\n",
    "\n",
    "# customisation : gpt-4 instead of gpt-3.5-turbo\n",
    "# VectorStoreIndex will use gpt-4 to encode or embed your documents for later querying\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "llm - OpenAI(\n",
    "    temperature = 0.1,\n",
    "    model='gpt-4'\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available LLMs**\n",
    "\n",
    "Integrations with OpenAI, HuggingFace, PaLM, etc. You can also run a model like Llama2 locally. Use it in the ServiceContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. using llama2 locally\n",
    "# this uses llama2-chat-13b from LlamaCPP\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Privacy and Security**\n",
    "\n",
    "By default, LlamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, this can be configured according to our preferences. LlamaIndex provides the flexibility to use our own embedding model or run a LLM locally is desired.\n",
    "\n",
    "Privacy and handling of your data are subject to OpenAI's policies. LlamaIndex offers modules to connect with other vector stores within indexe to store embeddings. It is worth noting that each vector store has its own privacy policies and practices. Also be default, LlamaIndex has an option to store your embeddings locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the proper LLM is the first step in building the application. LLMs are a core component, they can be used as a standalone module or plugged into other core LlamaIndex modules like indices, retrievers, query engines, etc. They are always used in the responses synthesis step, i.e. after retrieval. Depending on the type of index being used, LLMs may be also be used during index construction, insertion and query traversal.\n",
    "\n",
    "LlamaIndex provides a unified interface for defining LLM modules, whether it is from OpenAI, HuggingFace or LangChain. It consists of the following - \n",
    "1. Support for text completion and chat\n",
    "2. Support for streaming and non-streaming endpoints\n",
    "3. Support for synchronous and asynchronous endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-streaming\n",
    "response = OpenAI().complete('Paul Graham is ')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LLMs as standalone modules**\n",
    "\n",
    "1. Text completion example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import OpenAI\n",
    "\n",
    "# non-streaming\n",
    "response = OpenAI().complete('Paul Graham is ')\n",
    "print(response)\n",
    "\n",
    "# using streaming endpoint\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "resp = llm.stream_complete('Paul Graham is ')\n",
    "for delta in resp:\n",
    "    print(delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Chat example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, OpenAI\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role = 'system',\n",
    "        content = 'You are a helpful assistant'\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role = 'user',\n",
    "        content = 'what is your name?'\n",
    "    ),\n",
    "]\n",
    "\n",
    "resp = OpenAI().chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customizing LLMs with LlamaIndex Abstractions**\n",
    "\n",
    "You can plugin these abstractions within our other modules like indexes, retrievers, query engines, agents, etc. allowing to build advacned workflows over our data.\n",
    "\n",
    "*Customisation examples:*\n",
    "1. Changing the underlying LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature = 0.1,\n",
    "    model = 'gpt-4'\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Changing the number of output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for OpenAI, Cohere and AI21\n",
    "llm = OpenAI(\n",
    "    temperature = 0,\n",
    "    model = 'text-davinci-002',\n",
    "    max_tokens = 512    # maxTokens for AI21\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "# for other LLM classes, configure the `context_window` and `num_output` explicitly via the `ServiceContext`,\n",
    "# as this information is not available by default\n",
    "\n",
    "context_window = 4096\n",
    "num_output = 256\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature = 0,\n",
    "    model = 'text-davinci-002',\n",
    "    max_tokens = num_output\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm = llm,\n",
    "    context_window = context_window,\n",
    "    num_output = num_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using a HuggingFace LLM : LlamaIndex supports using LLMs from HuggingFace directly, also you can setup a local embedding model. Many open-source models require some preamble before each prompt, which is a `system_prompt`. Additionally, queries may need an additional wrapper around the `query_str` itself. All this is available on the model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = '''<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "'''\n",
    "\n",
    "# this wraps the default prompts that are internal to llama_index\n",
    "query_wrapper_prompt = PromptTemplate('<|USER|>{query_str}<|ASSISTANT|>')\n",
    "\n",
    "import torch\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens = 256,\n",
    "    generate_kwargs = {\n",
    "        'temperature' : 0.7,\n",
    "        'do_sample' : False\n",
    "    },\n",
    "    system_prompt = system_prompt,\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    tokenizer_name = 'StabilityAI/stablelm-tuned-alpha-3b',\n",
    "    model_name = 'StabilityAI/stablelm-tuned-alpha-3b',\n",
    "    device_map = 'auto',\n",
    "    stopping_ids = [50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs = {\n",
    "        'max_length' : 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size = 1024,\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models may raise error is all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`. Here is how to configure the predictor to remove it -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuggingFaceLLM(\n",
    "    #...\n",
    "    tokenizer_outputs_to_remove = ['token_type_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "By default, LlamaIndex uses a global tokenizer for all token counting. This defaults to `cl100k1` from tiktoken, to match the default LLM `gpt-3.5-turbo`. If you change the LLM, you may need to update the tokenizer to ensure accurate token counts, chunking and prompting. The single requirement for a tokenizer is that it is a callable function, that takes a string and returns a list. You can also set a global tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import set_global_tokenizer\n",
    "\n",
    "# tiktoken\n",
    "import tiktoken\n",
    "set_global_tokenizer(tiktoken.encoding_for_model('gpt-3.5-turbo').encode)\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import AutoTokenizer\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta').encode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open-Source LLMs**\n",
    "\n",
    "1. [llama2-chat-7b-4bit](https://colab.research.google.com/drive/14N-hmJ87wZsFqHktrw40OU6sVcsiSzlQ?usp=sharing)\n",
    "2. [llama2-chat-13b (replicate)](https://colab.research.google.com/drive/1S3eCZ8goKjFktF9hIakzcHqDE72g0Ggb?usp=sharing)\n",
    "3. [llama2-70b-chat (replicate)](https://colab.research.google.com/drive/1BeOuVI8StygKFTLSpZ0vGCouxar2V5UW?usp=sharing)\n",
    "4. [Mistral-7b-instruct-v0.1 4bit](https://colab.research.google.com/drive/1ZAdrabTJmZ_etDp10rjij_zME2Q3umAQ?usp=sharing)\n",
    "5. [zephyr-7b-alpha](https://colab.research.google.com/drive/16Ygf2IyGNkb725ZqtRmFQjwWBuzFX_kl?usp=sharing)\n",
    "6. [zephyr-7b-beta](https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing)\n",
    "7. [stablelm-zephyr-3b](https://colab.research.google.com/drive/1USBIOs4yUkjOcxTKBr7onjlzATE-974T?usp=sharing)\n",
    "8. [starling-lm-7b-alpha](https://colab.research.google.com/drive/1Juk073EWt2utxHZY84q_NfVT9xFwppf8?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module Integrations**\n",
    "\n",
    "1. HuggingFace LLM - Camel-5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "# download data\n",
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n",
    "\n",
    "# load documents, build the VectorStoreIndex\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# setup prompts - specific to StableLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# model - 1\n",
    "# setup prompts : for StableLM, taken from model card\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "# model - 2 \n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "# taken from https://huggingface.co/Writer/camel-5b-hf\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=2048,    #4096\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Writer/camel-5b-hf\",    #StabilityAI/stablelm-tuned-alpha-3b\n",
    "    model_name=\"Writer/camel-5b-hf\",\n",
    "    device_map=\"auto\",\n",
    "    #stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512, llm=llm) #1024\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "# query index\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)\n",
    "\n",
    "# query index - streaming\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "response_stream = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "# can be slower to start streaming since llama-index often involves many LLM calls\n",
    "response_stream.print_response_stream()\n",
    "\n",
    "# can also get a normal response object\n",
    "response = response_stream.get_response()\n",
    "print(response)\n",
    "\n",
    "# can also iterate over the generator yourself\n",
    "generated_text = \"\"\n",
    "for text in response.response_gen:\n",
    "    generated_text += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Local Llama2 + VectorStoreIndex : GPU, A100 with at least 40GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "\n",
    "# login to HF hub with access to llama2 models\n",
    "# using 'huggingface-cli login' in console\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import torch\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# Model names (make sure you have access on HF)\n",
    "LLAMA2_7B = \"meta-llama/Llama-2-7b-hf\"\n",
    "LLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "LLAMA2_13B = \"meta-llama/Llama-2-13b-hf\"\n",
    "LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "LLAMA2_70B = \"meta-llama/Llama-2-70b-hf\"\n",
    "LLAMA2_70B_CHAT = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "selected_model = LLAMA2_13B_CHAT\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=2048,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=selected_model,\n",
    "    model_name=selected_model,\n",
    "    device_map=\"auto\",\n",
    "    # change these settings below depending on your GPU\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
    ")\n",
    "\n",
    "# download data\n",
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "documents\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    set_global_service_context,\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en\"\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# querying\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "# streaming support\n",
    "import time\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"What happened at interleaf?\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "token_count = 0\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "    token_count += 1\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "tokens_per_second = token_count / time_elapsed\n",
    "\n",
    "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. LlamaCPP : Highly configurable, depending on the model used, you may want to pass in `messages_to_prompt` and `completion_to_prompt` functions to help format the model inputs. Default model is llama2-chat, we can use util functions in `llama_index.llms.llama_utils`, any kwargs during initialization to be passed in `model_kwargs`, any kwargs during inference to be passed in `generate_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# chat completions\n",
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)\n",
    "\n",
    "# stream_complete\n",
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)\n",
    "\n",
    "# query engine with llamacpp\n",
    "# we can simply pass the llm abstraction to the query engine as usual\n",
    "# but first, let is change the global tokenizer to match the LLM\n",
    "from llama_index import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")\n",
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# create a service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../../examples/paul_graham_essay/data\"\n",
    ").load_data()\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Using LlamaIndex with local models using GPT4ALL](https://colab.research.google.com/drive/16QMQePkONNlDpgiltOi7oRQgmB8dU5fl?usp=sharing#scrollTo=20cf0152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Embeddings</h1></center>\n",
    "\n",
    "Embeddings are used to represent the documents using a sophisticated numerical representation. Take text as input and return a long list of numbers used to capture the semantics of the text. When calculating similarity, by default cosine similarity is used. Default model is `text-embedding-ada-002` from OpenAI.\n",
    "\n",
    "**Usage Pattern**\n",
    "\n",
    "Most common usage is setting it in the service context object and then using it to construct the index and query. Input documents will be broken down into nodes and the embedding model will generate an embedding for each node. Optionally you can set a global service context to avoid passing it into other objects every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "# local model\n",
    "service_context = ServiceContext.from_defaults(embed_model='local')\n",
    "\n",
    "# global service context\n",
    "from llama_index import set_global_service_context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customization**\n",
    "\n",
    "1. Batch size : default 10, may incur a rate limit, may be too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size to 42\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model = 'local:BAAI/bge-large-en'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. HuggingFace Optimum ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers optimum[exporters]\n",
    "\n",
    "from llama_index.embeddings import OptimumEmbedding\n",
    "OptimumEmbedding.create_and_save_optimum_model(\n",
    "    'BAAI/bge-small-en-v1.5',\n",
    "    './bge_onnx'\n",
    ")\n",
    "\n",
    "embed_model = OptimumEmbedding(folder_name = './bge_onnx')\n",
    "service_context = ServiceContext.from_defaults(embed_model = embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Embeddings with HuggingFace**\n",
    "\n",
    "Support for BGE, Instructor and more. Also to create and use ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Embedding\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name = 'BAAI/bge-small-en-v1.5'\n",
    ")\n",
    "\n",
    "embeddings = embed_model.get_text_embedding('Hello World!')\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor Embedding\n",
    "from llama_index.embeddings import InstructorEmbedding\n",
    "\n",
    "embed_model = InstructorEmbedding(\n",
    "    model_name = 'hkunlp/instructor-base'\n",
    ")\n",
    "\n",
    "embeddings = embed_model.get_text_embedding('Hello World!')\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# HF\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# needed to synthesize responses later\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "%%timeit -r 1 -n 1\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context, show_progress=True\n",
    ")\n",
    "\n",
    "# ONNX\n",
    "from llama_index.embeddings import OptimumEmbedding\n",
    "\n",
    "embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")\n",
    "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "%%timeit -r 1 -n 1\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading Data (Ingestion)<h1></center>\n",
    "\n",
    "Before your LLM can act on the data, you need to process it and load it. It has 3 stages :\n",
    "1. Load the data\n",
    "2. Transform the data\n",
    "3. Index and store the data\n",
    "\n",
    "**Loaders**\n",
    "\n",
    "Data connectors, also called `Readers`. Ingest data from different sources and format them into `Document` objects, collection of data and metadata.\n",
    "\n",
    "Easiest way is using SimpleDirectoryReader. It can read a variety of formats including markdown, PDFs, word documents, powerpoint decks, images, audio and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('./data').load_data(show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading connector from LlamaHub - query against SQL database\n",
    "from llama_index import download_loader\n",
    "\n",
    "DatabaseReader = download_loader('DatabaseReader')\n",
    "\n",
    "reader = DatabaseReader(\n",
    "    scheme = os.getenv('DB_SCHEME'),\n",
    "    host = os.getenv('DB_HOST'),\n",
    "    port = os.getenv('DB_PORT'),\n",
    "    user = os.getenv('DB_USER'),\n",
    "    password = os.getenv('DB_PASS'),\n",
    "    dbname = os.getenv('DB_NAME'),\n",
    ")\n",
    "\n",
    "query = 'SELECT * FROM users'\n",
    "documents = reader.load_data(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "GoogleDocsReader = download_loader('GoogleDocsReader')\n",
    "loader = GoogleDocsReader()\n",
    "documents = loader.load_data(\n",
    "    document_ids = [...]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.web.simple_web.base import SimpleWebPageReader\n",
    "\n",
    "reader = SimpleWebPageReader(html_to_text=True)\n",
    "docs = reader.load_data(\n",
    "    urls=['']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating documents directly\n",
    "from llama_index.schema import Document\n",
    "doc = Document(text='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations**\n",
    "\n",
    "After loading, data has to be processed and transformed before putting it into a storage system. These include chunking, extracting metadata and embedding each chunk. This is necessary to make sure the data can be retrieved and used optimally by the LLM. Transformation inputs/outputs are Node objects (Document is a subclass of Node). They can be stacked and reordered.\n",
    "\n",
    "Document is a generic container around any data source. Can be constructed manually or created automatically via the data loaders. Data Loaders return Document objects via the load_data function. By default, a Document stores text along with some attributes like metadata and relationships. \n",
    "\n",
    "Node is a chunk of a source document, they also contain metadata and relationsip inforkmation with other nodes. You may choose to parse source Documents into Nodes through the NodeParser classes.\n",
    "\n",
    "Customising Documents :\n",
    "Using the metadata dictionary on each document, additional information can be included to help inform responses and track down sources for query responses. Any information set in the metadata dictionary will show up in the metadata of each source node created from the document. This includes filename and doc_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name\n",
    "\n",
    "# 1. in constructor\n",
    "document = Document(\n",
    "    text = 'text',\n",
    "    metadata = {\n",
    "        'filename' : '<doc_finename',\n",
    "        'category' : '<doc_category'\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. after document is created\n",
    "document.metadata = {\n",
    "    'filename' : '<doc_filename'\n",
    "}\n",
    "\n",
    "# 3. set filename automatically\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "filename_fn = lambda filename : {'file_name' : filename}\n",
    "documents = SimpleDirectoryReader(\n",
    "    './data',\n",
    "    file_metadata = filename_fn\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_id\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    './data',\n",
    "    filename_as_id=True\n",
    ").load_data\n",
    "print([x.doc_id for x in documents])\n",
    "\n",
    "# set it manually\n",
    "document.doc_id = 'My new document id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "High-level API : `.from_documents()` method of indexes accept an array of Document objects and correctly parse and chunk them up. This splits the Document into Node objects, similar to Documents but have a relationship to parent document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in a custom ServiceContext to customize core components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "# load documents first\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 10\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-level API : You can also define these steps explicitly, either using the transformation modules (text splitters, metadata extractors, etc.) as standalone components or compose them into our declarative Transformation pipeline interface.\n",
    "\n",
    "Splitting documents into nodes : split documents into chunks. Key idea is to process data into bite-sized pieces that can be retrieved / fed to the LLM. Can be used on their own or as part of the ingestion pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "docs = SimpleDirectoryReader('./data').load_data()\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        TokenTextSplitter(),\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "nodes = pipeline.run(documents=docum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding metadata : Either manually or with automatic metadata extractors.\n",
    "\n",
    "You can use LLMs to automate metadata extraction with the Metadata Extractor modules. It includes the following feature extractors:\n",
    "1. SummaryExtractor - automatically extract a summary over a set of nodes\n",
    "2. QuestionsAnsweredExtractor - extracts a set of questions that each Node can answer\n",
    "3. TitleExtractor - extracts a title over the context of each Node\n",
    "4. EntityExtractor - extracts entities mentioned in the context of each Node\n",
    "\n",
    "Then you can chain the Metadata Extractors with the node parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor\n",
    ")\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=' ',\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "title_extractor = TitleExtractor(nodes=5)\n",
    "qa_extractor = QuestionsAnsweredExtractor(questions=3)\n",
    "\n",
    "# assume documents are defined -> extract nodes\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor,\n",
    "        qa_extractor\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=documents,\n",
    "    in_place=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automated Metadata Extraction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "\n",
    "from llama_index.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.extractors import (\n",
    "    SummaryExtractor,\n",
    "    TitleExtractor,\n",
    "    EntityExtractor\n",
    ")\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "extractor = [\n",
    "    SummaryExtractor(\n",
    "        summaries=['prev', 'self', 'next'],\n",
    "        service_context=service_context\n",
    "    ),\n",
    "    TitleExtractor(\n",
    "        nodes=5\n",
    "    ),\n",
    "    EntityExtractor(\n",
    "        prediction_threshold=0.5,\n",
    "        label_entities=False,\n",
    "        device='cpu'\n",
    "    )\n",
    "]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[splitter, *extractor]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=documents,\n",
    "    in_place=False,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "engine = index.as_query_engine(\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
    "\n",
    "<center><h1>Query Engine<h1></center>\n",
    "\n",
    "Generic interface that allows to ask questions over our data. Takes in a natural language query and returns a rich response. It is most often built on one or mroe indexes via retrievers. You can compose multiple query engines to achieve more advanced capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage pattern\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query('')\n",
    "\n",
    "# to stream response\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query('')\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuring a query engine**\n",
    "\n",
    "Directly build and configure from an index as follows. Response modes currently supported :\n",
    "1. `refine` - \n",
    "* create and refine the answer by sequentially going through each retrieved text chunk\n",
    "* makes separate LLM call per node/retrieved chunk\n",
    "* first chunk is used using the `text_qa_template` prompt, answer and next chunk used in another query with `refine_template` prompt, and so on\n",
    "* too large chunk is split using `TokenTextSplitter`\n",
    "* good for more detailed answers\n",
    "\n",
    "\n",
    "2. `compact` -\n",
    "* default, similar to `refine` but concatenates the chunks beforehand, resulting in less LLM calls\n",
    "* stuff as many chunks that can fit in the context window\n",
    "\n",
    "3. `tree_summarize` -\n",
    "* query LLM using `summary_template` as many times as needed so that all concatenated chunks have been queries, resulting in as many answers\n",
    "* good for summarization\n",
    "\n",
    "4. `simple_summarize`\n",
    "5. `no_text`\n",
    "6. `accumulate`\n",
    "7. `compact_accumulate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    response_mode='tree_summarize',\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low-level composition API**\n",
    "\n",
    "granular control, explicitly construct a query engine object instead of calling `index.as_query_engine()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode='tree_summarize',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "response = query_engine.query('')\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using indexes if cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./citation\"):\n",
    "    documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context\n",
    "    )\n",
    "    index.storage_context.persist(persist_dir=\"./citation\")\n",
    "else:\n",
    "    index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./citation\"),\n",
    "        service_context=service_context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CitationQueryEngines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.query_engine import CitationQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    ServiceContext,\n",
    ")\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    ")\n",
    "\n",
    "\n",
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n",
    "\n",
    "if not os.path.exists(\"./citation\"):\n",
    "    documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context\n",
    "    )\n",
    "    index.storage_context.persist(persist_dir=\"./citation\")\n",
    "else:\n",
    "    index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./citation\"),\n",
    "        service_context=service_context,\n",
    "    )\n",
    "\n",
    "\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    # here we can control how granular citation sources are, the default is 512\n",
    "    citation_chunk_size=512,\n",
    ")\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)\n",
    "\n",
    "# source nodes are 6, because the original chunks of 1024-sized nodes were broken into more granular nodes\n",
    "print(len(response.source_nodes))\n",
    "\n",
    "\n",
    "print(response.source_nodes[0].node.get_text())\n",
    "\n",
    "print(response.source_nodes[1].node.get_text())\n",
    "\n",
    "\n",
    "# adjusting settings\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    # increase the citation chunk size!\n",
    "    citation_chunk_size=1024,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)\n",
    "\n",
    "# should be less source nodes now!\n",
    "print(len(response.source_nodes))\n",
    "\n",
    "# inspecting the actual source\n",
    "print(response.source_nodes[0].node.get_text())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
