{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff7db9e-fbf9-4394-9958-35323799a4e3",
   "metadata": {
    "id": "dff7db9e-fbf9-4394-9958-35323799a4e3"
   },
   "source": [
    "<center><h1>Retrieval-Augmented Generation (RAG) Pipeline for Market Research and Analytics</h1></center>\n",
    "\n",
    "We intend to deploy RAG-based AI for streamlining market research and data analysis. This system uses __'Llama-2-7B'__ as the underlying LLM and __'HuggingFace Sentence Transformers'__ as the embedding model. The system utilises document retrieval-based text generation, with source citations. This produces trustworthy and credible responses, thus eliminating hallucinations, which is a major cause of concern with the Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337aa8b4",
   "metadata": {},
   "source": [
    "> #### __Creating the virtual environment and installing dependencies__\n",
    "\n",
    "Creating a virtual environment for every new project is very crucial to avoid any clashes with the dependencies of the software packages. Next, we install the list of software packages and dependencies from the text file `requirements.txt` present in the same project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693def98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new virtual environment named '.venv'\n",
    "#!python -m venv RAG    # uncomment code if needed\n",
    "#!RAG\\Scripts\\activate  # uncomment code if needed\n",
    "\n",
    "# install dependencies\n",
    "#!pip install -r requirements.txt   # uncomment code if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63935557-a11c-4a22-9248-9c746cc89c4c",
   "metadata": {
    "id": "63935557-a11c-4a22-9248-9c746cc89c4c"
   },
   "source": [
    "> #### __The LLM : Llama2-7B-chat.Q6_K__\n",
    "\n",
    "* Open-source model, developed by 'Meta'.\n",
    "* Downloads last month : 962,379\n",
    "* Collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters\n",
    "* Use governed by the Meta license\n",
    "* Meta developed and publicly released the Llama 2 family of LLMs\n",
    "* The said model is optimized for dialogue use cases, outperform open-source chat models on most benchmarks and human evaluations on helpfulness and safety\n",
    "* Model architecture is an auto-regressive language model that uses an optimized transformer architecture, tuned versions use supervised fine-tuning (SFT) and reinforcement-learning with human feedback (RLHF) to align to human preferences for helpfulness and safety\n",
    "* Training data – a new mix of publicly available online data, 2 trillion tokens, includes publicly available instruction datasets as well as over one million new human-annotated examples, neither pretraining nor fine-tuning datasets include Meta user data\n",
    "* Data freshness – cutoff date is September 2022, some tuning data is more recent, upto July 2023\n",
    "* Parameters – 7B\n",
    "* Content length – 4k\n",
    "* Training duration – Jan 2023 to July 2023\n",
    "* Original paper : Llama2 : Open Foundation and Fine-tuned Chat Models. [Click here to read the paper.](https://arxiv.org/abs/2307.09288)\n",
    "\n",
    "We are running the model locally, using the Python bindings for the C++ library `Llama-CPP`\n",
    "\n",
    "Here is the link to the model card : [Llama2-7B-chat.Q6_K](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb975f6-c192-4a26-ae50-e9a319d2a66b",
   "metadata": {
    "id": "3cb975f6-c192-4a26-ae50-e9a319d2a66b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '18', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "\n",
    "model_path = r\"C:\\0-VARAD-DESHMUKH\\models\\llama-2-7b-chat.Q6_K.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_path=model_path,\n",
    "    temperature=0,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971bd41",
   "metadata": {},
   "source": [
    "> #### __The Embedding Model : BAAI/bge-large-en-v1.5__\n",
    "\n",
    "Open-source embedding model, developed by Beijing Academy of Artificial Intelligence. We download the model from the HuggingFace Model Hub. \n",
    "\n",
    "Here is the link to the model card : [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703308bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 779/779 [00:00<?, ?B/s] \n",
      "c:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rck05\\AppData\\Local\\llama_index\\models--BAAI--bge-large-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [01:00<00:00, 22.2MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 633kB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 949kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 137kB/s]\n"
     ]
    }
   ],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273ff8e-7b9a-427a-ad1d-aa3099e9d9b8",
   "metadata": {
    "id": "3273ff8e-7b9a-427a-ad1d-aa3099e9d9b8"
   },
   "source": [
    "> #### __Define Service Context__\n",
    "\n",
    "We need to configure the system by specifying the llm and the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1746a171-5b20-4853-8574-71f1547bce30",
   "metadata": {
    "id": "1746a171-5b20-4853-8574-71f1547bce30"
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a762729",
   "metadata": {},
   "source": [
    "> #### __Load the Data__\n",
    "\n",
    "We first load the `PDFReader` from `LlamaHub`, specify the path to our PDF document and construct the Document object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77d9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionl\n",
    "import os\n",
    "static_dir = 'static/'\n",
    "if not os.path.exists(static_dir):\n",
    "   os.mkdir(static_dir)\n",
    "\n",
    "# loading the PDFReader from llama_index hub\n",
    "from llama_index import download_loader\n",
    "\n",
    "# PyMuReader\n",
    "PyMuPDFReader = download_loader('PyMuPDFReader')\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "pdf_path = r\"C:\\0-VARAD-DESHMUKH\\Files\\data\\Report.pdf\"\n",
    "\n",
    "documents = loader.load_data(\n",
    "    file_path=pdf_path,\n",
    "    metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405dd8f2",
   "metadata": {},
   "source": [
    "> #### __Local Vector Store__\n",
    "\n",
    "After we load the documents, we setup a local Vector Index store, for storing the vector embeddings of the source documents as well as the query, for efficient semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393fa25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 27/27 [00:00<00:00, 1265.96it/s]\n",
      "Generating embeddings: 100%|██████████| 27/27 [01:13<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        show_progress=True,\n",
    "        service_context=service_context\n",
    "    )\n",
    "    # store it for later\n",
    "index.storage_context.persist(persist_dir=PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2067b7d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\llms\\utils.py:29\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm)\u001b[0m\n\u001b[0;32m     28\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\llms\\openai_utils.py:381\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# load the existing index\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(persist_dir\u001b[38;5;241m=\u001b[39mPERSIST_DIR)\n\u001b[1;32m---> 29\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mload_index_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\indices\\loading.py:33\u001b[0m, in \u001b[0;36mload_index_from_storage\u001b[1;34m(storage_context, index_id, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     index_ids \u001b[38;5;241m=\u001b[39m [index_id]\n\u001b[1;32m---> 33\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mload_indices_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m     )\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\indices\\loading.py:78\u001b[0m, in \u001b[0;36mload_indices_from_storage\u001b[1;34m(storage_context, index_ids, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m index_struct\u001b[38;5;241m.\u001b[39mget_type()\n\u001b[0;32m     77\u001b[0m     index_cls \u001b[38;5;241m=\u001b[39m INDEX_STRUCT_TYPE_TO_INDEX_CLASS[type_]\n\u001b[1;32m---> 78\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mindex_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     indices\u001b[38;5;241m.\u001b[39mappend(index)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:53\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, service_context, storage_context, use_async, store_nodes_override, insert_batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\indices\\base.py:63\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes must be a list of Node objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context \u001b[38;5;241m=\u001b[39m service_context \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context \u001b[38;5;241m=\u001b[39m storage_context \u001b[38;5;129;01mor\u001b[39;00m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mdocstore\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\service_context.py:178\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMPredictor is deprecated, please use LLM instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mLLMPredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpydantic_program_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_program_mode\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[0;32m    182\u001b[0m     llm_predictor\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\llm_predictor\\base.py:109\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[1;34m(self, llm, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m     llm: Optional[LLMType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m     pydantic_program_mode: PydanticProgramMode \u001b[38;5;241m=\u001b[39m PydanticProgramMode\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[0;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback_manager:\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[1;32mc:\\0-VARAD-DESHMUKH\\.venv\\Lib\\site-packages\\llama_index\\llms\\utils.py:31\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm)\u001b[0m\n\u001b[0;32m     29\u001b[0m         validate_openai_api_key(llm\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     42\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    get_response_synthesizer\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "document_directory = r\"C:\\0-VARAD-DESHMUKH\\Files\\data\"\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = r\"C:\\0-VARAD-DESHMUKH\\storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(input_dir=document_directory).load_data()\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        show_progress=True,\n",
    "        service_context=service_context\n",
    "    )\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436f245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574a062-1900-4b74-be9a-6248ffb8bbbe",
   "metadata": {
    "id": "d574a062-1900-4b74-be9a-6248ffb8bbbe"
   },
   "source": [
    "> #### __The Retrieval Pipeline__\n",
    "\n",
    "We now build a retrieval pipeline. We write a query in the form of a prompt, convert it into an embedding as well and then query the vector database based on that query. This query will just retrieve the relevant nodes from the source document. These filtered out nodes will subsequently be used to answer the user queries based on that retrieved material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba6d9f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The alternative models being considered for fine-tuning are BERT, RoBERTa, and DistilBERT. These models have shown promising results in various NLP tasks and are widely used in the field. However, due to their complexity and computational requirements, they are not feasible for our specific use case.\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "\n",
    "prompt = '''\n",
    "What model alternatives are being considered? Answer strictly according to the source document. Keep the tone technical. Answer strictly in 100 words.'''\n",
    "\n",
    "response = query_engine.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7838289-5a59-4042-b4b7-037f66d99be4",
   "metadata": {
    "id": "f7838289-5a59-4042-b4b7-037f66d99be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 | R A G - b a s e d  A I  s o l u t i o n  f o r  I n v e s t m e n t  R e s e a r c h  a n d  A n a l y t i c s  \n",
      " \n",
      "So, our take is that for our specific use case, fine-tuning a model is not a feasible option. We need \n",
      "something which is cost-efficient and saves time and resources. So, we look at the other, widely used \n",
      "alternative, retrieval-based approaches. We take this up next.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
